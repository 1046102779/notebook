**学习永不止步**

#### 一致性hash

我是两次遇到hash一致性相关问题：
1. 需要设计电单车的智能锁平台，如果线上有百万个锁接入锁平台，考虑到高可用，一个tcp服务存在单点问题；则需要设计支持百万量级的实时在线管理平台，所以需要在接入层考虑如何把锁全部均衡的接入到各个锁服务平台，同时如果一个服务挂掉，则怎样使锁离线重连抖动最小，则考虑了在nginx层引入hash一致性；
2. Amazon的Dynamo去中心化的高键值存储系统，依赖hash一致性算法。

我看了第一篇，写得很不错

[一致性hash算法](https://blog.csdn.net/sparkliang/article/details/5279393)

[白话解析：一致性hash算法](http://www.zsythink.net/archives/1182)

```shell
总结：
1.通过环的引入和hash计算节点值和目标值的映射，解决雪崩问题；
2.引入中间层虚拟节点列表，使得节点上存储的数据尽量平衡
```

## [互联网技术架构](https://github.com/davideuler/architecture.of.internet-product)

### 微博架构

看了微博架构的全部文章，[微博数据库那些事儿:3个变迁阶段背后的设计思想](https://github.com/davideuler/architecture.of.internet-product/blob/master/2.%E5%BE%AE%E5%8D%9A%E6%9E%B6%E6%9E%84/%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%EF%BC%9A3%E4%B8%AA%E5%8F%98%E8%BF%81%E9%98%B6%E6%AE%B5%E8%83%8C%E5%90%8E%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3.pdf) 一文，两个点自己看明白了

```shell
1. 微博博文存储面临巨大挑战。
    (1). 首先，我们将索引同内容进行了拆分，因为索引所需存储空间较少，而内容存储所需空间较大， 且这两者的使用需求也不尽相同，访问频次也会不同，需要区别对待。
    (2). 然后，分别对索引和内容采用先 hash，再按照时间维度拆分的方式进行水平拆分，尽量保障每张 表的容量在可控范围之内，以保证查询的性能指标。
    (3). 最后，业务先通过索引获得实际所需内容的 id，再通过内容库获得实际的内容，并通过部署 memcached 来加速整个过程，虽然看上去步骤变多，但实际效果完全可以满足业务需求。
    
    后面作者也回答了这个问题。
    (1). 针对博文时间强相关，内容数据量大的特点。把内容和索引垂直拆分成两类表，一类存储索引和博文ID，一类存储博文ID和博文内容数据；
    (2). 我们针对索引表的时间强相关特性，我们用redis做缓存，以及索引表按时间特性水平拆分，DATETIME(now-expire)则进行归档；同时博文表也进行mod%1024值(1024：表示博文拆分表的总数量)， 这1024个表也具有时间特性，整体超过某个时间，也进行归档。
    (3). 通过缓存加速索引过程。

2. 印象最深的一次数据库服务故障能否回忆并说几点注意事项?
    一同事不小心执行了drop table命令，导致线上服务受影响。所以做了一个删表需求流程， 具体如下：
    (1). 执行 rename table 操作，将 table rename 成 table—will-drop。 
    (2). 等待 24 小时之后再执行 drop 操作。
```

### 阿里架构

[2017阿里技术精选](https://github.com/davideuler/architecture.taobao-alibaba/blob/b8bb7bf3046026722cbe536f47ee5bd87a9d7e92/2017%20%E9%98%BF%E9%87%8C%E6%8A%80%E6%9C%AF%E5%B9%B4%E5%BA%A6%E7%B2%BE%E9%80%89%20Alitech%20Archive_1.pdf)  有两篇文章能看懂一些知识：
```shell
1. 《阿里下一代数据库技术：把数据库装入容器不再是神话》写得能看懂一些.
    文章针对mysql的innodb存储引擎读写比较均衡，底层是B+Tree，文章中有一句话写得相当到位，“或者动不动 DBA 去找业务开发方 说你的存储空间不够了，占用很多空间，能不能删一些数据或者把这些数据导入到成本 更低的存储引擎里。我们经常这么干，这里说的直白一点，我相信大家都这么干过。”，我也遇到过，所以阿里团队想利用rocksdb的压缩率高的特性，rocksdb存储引擎是使用的LSMTree(Log based Structure Merge Tree)，相比较mysql的innodb引擎，特点是写快、且顺序写入。利用rocksdb压缩率高的特性，使得可以存储时间更长的数据，减少冷热数据的用户感知。 我有一点非常好奇，怎么把这两种引擎相结合并同时使用呢？
    
    数据库实现弹性调度，以前觉得纯粹扯淡，因为数据库是资源存储，所以一般不会移动。但是作者提到了实现弹性调度的两个前提：1. 数据库容器化；2. 计算和存储分离；对于后者，计算和存储之间的网络通信必须是可接受的
2.  《如何打造千万级Feed流系统？阿里数据库技术解读》中提到的PG数据库(PostgreSQL)，说它的读写并发量大，所以我查了它的特性和应用场景。特性：稳定、性能好，（查询、类型、功能和插件）丰富；应用场景：LBS、OLAP实时在线分析。
3. 《深度解读|阿里云新一代关系型数据库PolarDB》，这篇文章介绍了PolarDB优秀之处，在于使用了现代非常多的技术特性，包括：分布式架构、数据库高可用、网络协议、存储块设备、文件系统和虚拟化。PolarDB把计算和存储完全分离，计算上建立索引、缓存等特性；存储使用块设备存储，并且采用业界流行的3节点备份，防止数据丢失。块设备节点同步通过改造后的并行raft算法实现AP和最终一致性，计算和存储通过RDMA协议使用高速网络进行通信。并且100%兼容mysql标准，同时读写速度比mysql快6倍
4. 《接下时序数据库存储的挑战书，阿里HiTSDB诞生了》， 这篇文章我收获颇多，写得非常好。因为我了解一点点InfluxDB、Prometheus和小米的Open-falcon，这篇文章提到了时序数据库的定位：插值、降精度和聚合。其中，插值：某个时间点数值缺失，这需要插值；降精度：采样打点，每秒1个，则呈现一年的数据，呈现所有点不可能，则把精度提高到1天的时间维度；聚合：需要做一些报表统计；第二个收获是由浅入深的讲解了为什么时间序列用RDBMS不合适，这就提到了B+ Tree、LSM Tree和SSTable。B+ Tree的特点是随机读、随机写。时间复杂度为LOG(N); 同时叶子节点是链表结构。文章提到如果每秒300W打点写入，这种索引树无法抗住，因为索引树再磁盘上、同时随机写入速度慢；所以希望时序数据存储是顺序写入，这种写入时间复杂度是O(1)， 这样速度大大提高，同时为了减少磁盘IO，最好是内存索引；又因为索引过大，不可能全部存储在内存中，所以希望部分所以在内存中计算，所以想引入LSM Tree(Log Based Structure Merge Tree),  若干个小索引在内存中，同时是顺序写。

降精度特别难优化，因为降精度是在时间维度上做的，首先要把时间序列维度拿出来然后再中间插值，而实际上SQL是不知道这件事情的，SQL是按点来操作的。所以RDBMS不适合做时序数据库的存储和计算。

所以引入基于LSM Tree的MyRocks存储引擎，顺序写入性能每秒20W点。但是这个方案的缺点：
    1. tag重复存储，存储开销大，压缩率不高；
    2. MyRocks强调事务，内部锁实现拖慢了简单写的性能；
    3. 由于多个索引降低了写入速度，最终系统的写性能大概5~6W点每秒；
    4. others

Elastic Search的时序存储方案。我没看明白，对ELK不熟悉。

基于列式存储的时序存储方案。产品有：Druid、inforbright。前者好像小米在使用。行式存储的update和insert操作方便、快，但是select慢，需要遍历所有索引或者存储内容；列式存储的列式存放在一起的，所以索引的建立非常方便、然后select方便快速，但是update和insert比较麻烦, 同时后者压缩率非常高。 作者说列式存储有些时序场景不太适合，因为列式存储是把导入的数据累积到一定程度，才会把一个包把它固定到磁盘上，但是时序数据如果查询以前的老数据，这意味着要查该时间段内的每一个包。

基于流引擎的时序存储方案。JStorm、Flink等，流引擎可以解决多维计算，它是个计算引擎，缺点：没有办法实时计算。

InfluxDB和OpenTSDB时序数据库最大特点，能把时间线提取出来，它的思路是事先做一系列的标签、先找到时间序列，在这个时间序列上再找到对应的点。时序数据是按照一个时间长度分片压缩存储在一起，这样搜索时不再需要搜索全部时间片段，搜索范围比较小。

最后作者选了OpenTSDB，HBase作为数据存储，可以保证它把一个小时的数据放到一个行里，这样压缩率比较高，可以做到每行数据20个字节左右，也就是说一个metric所对应一小时存储的字节数据可以压缩到20字节左右。
同时文章也指出OpenTSDB的劣势：
    1. 时间序列的Meta Data以缓存方式在所有的TSD节点上存在，数据量太多的时候，内存压力大；
    2. RowScan做非前缀查询时，会扫过很多毋庸的RowKey；
    3. 每行只存储一小时的打点数据，qualifier存在额外的开销；
    4. 单点聚合，容易出现聚合性能瓶颈
    5. 压缩率仍然不理想
后面团队根据这些缺点，在OpenTSDB的基础上，引入倒排索引、Facebook Gorilla高压缩比算法和分布式聚合引擎，推出HiTSDB时序数据库，这个部分我理解比较困难。
```

有关B+ Tree、LSM Tree和SSTable的介绍，有一篇文章写得很不错，推荐：[B+Tree VS. LSM](http://f.dataguru.cn/hadoop-29555-1-1.html)

有关行式存储和列式存储，推荐文章介绍：[一分钟理解列式存储](https://blog.csdn.net/dc_726/article/details/41143175)

OpenTSDB推荐一篇好文：[OpenTSDB通俗易懂的介绍](https://www.jianshu.com/p/0bafd0168647), 重点在OpenTSDB的存储方案上。
