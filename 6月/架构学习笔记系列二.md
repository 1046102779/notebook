## 阿里架构
[2017阿里技术精选](https://github.com/davideuler/architecture.taobao-alibaba/blob/b8bb7bf3046026722cbe536f47ee5bd87a9d7e92/2017%20%E9%98%BF%E9%87%8C%E6%8A%80%E6%9C%AF%E5%B9%B4%E5%BA%A6%E7%B2%BE%E9%80%89%20Alitech%20Archive_1.pdf) 中，一文《如何高效排查系统故障？一分钱引发的系统设计“踩坑”案例》讲解到， 理财年化收益率对账一分钱不平的问题，开发人员发现，在数据库记录中的收益转结记录中存在两笔转结收益，每日应该是一笔转结收益。

UserID | TX ID | 收益 | 创建时间 | 修改时间
---|---|---|---|---
用户A | TX ID A | 0.03 | 8:00:23 | 8:00:29
用户A | TX ID B | 0.03 | 8:00:29 | 8:00:29

开发人员发现这两笔结算时间挤在一起，怀疑很可能是出现了并发问题。继续跟踪第一笔“TX ID A”记录，开发确认线上日志存在超时情况，失败原因是数据库连接数打满，线程等待提交。

```shell
原有的系统设计思路：
1. 分布式锁有效时间5s；
2. sql操作发生错误，最多重试8次；
3. 分布式锁下的sql操作失败，重试操作会被拦截；
4. sql操作没有设置事务超时时间，或者超时时间过长

由此推断过程还原:
1. 分布式锁下的sql操作，因数据库连接数打满，导致线程请求等待，返回操作失败；则sql操作重试8次
2. 在分布式锁失效之前，所有的sql操作重试都被拦截；
3. 分布式锁5s后失效，然后重新发起幂等操作，这时5s已过，之前的8次重试没有执行完，数据库连接有空闲线程，这时会成功写入“TX ID A”记录，同时外围的幂等操作，会插入“TX ID B”记录；导致收益转结两次

过程抽象化：
1. 上层业务系统有重试机制；
2. 业务请求存在一定时间之后提交成功的情况；
3. 下游系统缺乏其他有效的幂等操作

解决方案：
1. 调整上游的重试时间， 且分布式锁有效期的时长大于前者；
2. 幂等操作的可靠性，比如，设置表索引的唯一性，发起每个用户每日转结该TX ID是唯一的。

其中增加幂等控制是值得推荐的。

我联想到：
如果通过一个TX ID（幂等操作）， 如果已有TX ID A，再来一个同样的 TX ID A，这个时候是状态更新的话，这会引出覆盖问题，状态覆盖的先后顺序问题，或者状态是否可逆问题？除非记录是无状态或者就是操作快照，则不会存在这个问题。
```


《纯干货|从淘宝到云端的高可用架构演进》

高可用的演进主要考察点：
1. 缓存设计
   ```shell
   在缓存设计上，最简单且高可用的部署模型：双机房。对缓存来说有两种经典部署模式：
   1. 共享集群部署。也就是说在IDC里，我的应用是分机房部署的，cache也是分机房部署的，对于应用来说，cache在逻辑上是一个集群，IDC1和IDC2各写一半cache。好处是，当一个IDC挂掉后，有一半cache是可以命中的，不会直接死掉。缓减服务器穿透压力。另外，减少了存储成本
   2. 独立部署。也就是说IDC1和IDC2的cache是一样的，IDC挂掉一个，cache照样命中，承担副本的作用。相对于共享集群部署，其缺点是：成本高，以及当一个IDC的某些cache失效时，另一个IDC的对应cache也要擦除，这样才能保证数据一致性 
   ```
   ```shell
   作者认为，一些东西都是可以被缓存的。我们再设计一个系统时，如果对数据一致性非常高，则会倾向于不用缓存，直接用数据库扛这个流量。 但是作者又举例说了一个问题：Mysql存储引擎InnoDB的Buffer Pool冷启动问题。异地多活面临一个问题，当建立一个新单元去承担这个机房的流量，流量切过来50%，这个单元会挂掉，因为mysql服务是冷的，缓存是空的，直接穿透整个数据存储。所以必须引入流控。
   同时现在业界有很多叫API网关(gateway)或者CDN，他们在边缘节点也做了一层短暂的cache，可能只cache50或者100ms，如果收到网络攻击，这层短暂的cache命中率是非常高的，后端服务可能只有几百个QPS，这个是防御网络攻击很好的手段。
   ```
2. 限流降级
    ```shell
    限流降级印象特别深刻的是有关Netflix的“防故障”基础设施：随机杀节点、延时响应和中断机房，这个是在正式环境做的，只能膜拜。
    
    对于限流降级的思考，最关键的地方在于，当发生故障后，你的服务是否会受到影响、同时需要多久时间进行服务恢复。
    降级服务通常的做法是服务回滚，但是有些服务启动的时间过长，这个过程对用户是有损的。如果有开关控制，则我可以在线上出现问题时，直接通过服务配置开关，把线上服务直接切到老服务上，所以开关降级的做法直接借鉴。
    ```
3. 容灾设计
    ```shell
    容灾设计的发生在于：server挂掉，比如：新引入的依赖、或者github上的package。 
    思路：如何降级、如何解决
    特别关注点：
        1. 是否有数据迁移，以及一致性的保证
        2. 服务发布顺序是否有依赖
        3. 是否需要停机
        4. 如何回滚
        5. 是否需要挂公告通知外部用户
    ```
4. 面向弹性的设计
    ```shell
    针对不同云服务的不同容灾策略，有个大原则：切  切  切，切流量。 第二个是购买虚拟机、负载均衡或者RDS时，一定要选择多可能去的服务，这样主备切流量，保证服务不受影响。
    ```
5. 面向跨区域/
    ```shell
    首先弄清楚两个概念，异地多活和跨区域。面向跨区域要解决的问题是，跨区域数据一致性问题、跨城市之间的数据传输耗时问题。
    ```
